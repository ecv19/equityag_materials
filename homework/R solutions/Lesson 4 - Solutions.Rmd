---
title: "Lesson 4 Solution Guide"
output: html_document
---

# Solution Guide: Lesson 4 — Variables, Transformations, and Robustness in Regression

## ✅ Part 1: Understanding Variables

### Example dummy variable creation:
```r
region <- factor(c("North", "South", "North", "South"))
dummies <- model.matrix(~ region)[, -1]  # Dummy for "South"
dummies
```
> This converts a categorical variable into a binary indicator (0/1), where "South" gets coded as 1.

### Ordinal variable caution:
```r
education <- ordered(c("High school", "College", "Graduate"),
                     levels = c("High school", "College", "Graduate"))
education
```
> This shows how ordered factors maintain rank. However, assuming equal spacing between levels could mislead regression.

### Likert-scale:
```r
likert <- ordered(c("Disagree", "Neutral", "Agree", "Strongly Agree"),
                  levels = c("Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"))
likert
```
> A common survey format treated as numeric for convenience but should be interpreted carefully.

### Continuous and Binary examples:
```r
income <- c(32000, 45000, 51000, 39000)
employed <- c(1, 0, 1, 1)
income
employed
```
> Continuous variables represent scale values, while binary variables encode true/false or yes/no as 1 and 0.

---

## ✅ Part 2: Distributions and Error Terms

### Fit model and plot residuals:
```r
model <- lm(mpg ~ wt, data = mtcars)
hist(residuals(model), breaks = 10, main = "Histogram of Residuals")
```
> This histogram helps visualize the distribution of errors. A bell shape indicates normally distributed residuals.

### Visualize standard errors:
```r
library(ggplot2)

ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE)
```
> The shaded area represents the 95% confidence interval around the regression line — based on standard errors.

### Compare OLS and median regression:
```r
ols_model <- lm(mpg ~ wt, data = mtcars)
summary(ols_model)

library(quantreg)
median_model <- rq(mpg ~ wt, data = mtcars, tau = 0.5)
summary(median_model)
```
> OLS minimizes mean squared error; quantile regression minimizes absolute deviations. Useful for skewed data.

---

## ✅ Part 3: Transforming Variables

### Standardize example:
```r
df <- data.frame(
  income = c(32000, 45000, 51000, 39000),
  education_years = c(12, 16, 18, 14))

df$income_std <- scale(df$income)
df$education_std <- scale(df$education_years)
df
```
> Standardization puts different units on the same scale — helpful when variables have different magnitudes.

### Log transform population:
```r
population <- c(800, 1200, 50000, 600, 90000)
log_pop <- log10(population)
barplot(log_pop, names.arg = c("A", "B", "C", "D", "E"))
```
> Without log, large values (like 90,000) would dominate the plot. This transformation compresses extreme values.

### Apply to `mtcars`:
```r
mtcars$wt_std <- scale(mtcars$wt)
mtcars$log_wt <- log(mtcars$wt)
mtcars$log_wt_safe <- log(mtcars$wt + 1)
head(mtcars)
```
> These are standard and log transformations of the car weight variable for analysis or regression.

---

## ✅ Part 4: When OLS Isn’t Enough

### Nonlinear fit:
```r
model_quad <- lm(mpg ~ wt + I(wt^2), data = mtcars)
summary(model_quad)
```
> Adds a squared term to capture curvature in the relationship between weight and mpg.

### Check heteroskedasticity:
```r
plot(model$fitted.values, model$residuals)
```
> If residuals fan out, error variance is not constant — violating OLS assumptions.

### Robust SE:
```r
library(sandwich)
library(lmtest)
coeftest(model, vcov = vcovHC(model, type = "HC1"))
```
> Produces standard errors that are robust to heteroskedasticity — improving inference.

### Logistic regression:
```r
mtcars$high_mpg <- ifelse(mtcars$mpg > 20, 1, 0)
logit_model <- glm(high_mpg ~ wt, data = mtcars, family = binomial)
summary(logit_model)
```
> Predicts the probability of high fuel efficiency using logistic regression (suitable for binary outcomes).

---

## ✅ Part 5: Robustness Checks

### Try different model spec:
```r
model_alt <- lm(mpg ~ wt + hp, data = mtcars)
summary(model_alt)
```
> Adds horsepower to the model to check if the relationship with mpg changes.

### Remove outlier:
```r
model_no_outlier <- lm(mpg ~ wt, data = mtcars[-which.max(mtcars$wt), ])
summary(model_no_outlier)
```
> Removes the heaviest car. If coefficients change a lot, results are sensitive to that outlier.

---

## 💡 Practice Tasks — Completed Examples with Explanations

### 1. Create different variable types
```r
region <- factor(c("East", "West", "East"))
edu_level <- ordered(c("HS", "College", "Grad"), levels = c("HS", "College", "Grad"))
income <- c(35000, 48000, 51000)
```
> This demonstrates how to store categorical, ordinal, and numeric variables.

### 2. Visualize with hist/boxplot, mean vs median
```r
hist(mtcars$mpg, main = "Histogram of mpg", col = "skyblue")
boxplot(mtcars$mpg, main = "Boxplot of mpg")
mean(mtcars$mpg)
median(mtcars$mpg)
```
> Histograms show distribution shape, boxplots highlight outliers and spread. Mean vs. median shows central tendency.

### 3. Apply `log()` and `scale()`
```r
log_hp <- log(mtcars$hp)
scaled_hp <- scale(mtcars$hp)
hist(log_hp, main = "Log-transformed HP")
hist(scaled_hp, main = "Standardized HP")
```
> Visualization confirms effects of transformation — reducing skew or centering.

### 4. Run `lm()` and add quadratic term
```r
model_basic <- lm(mpg ~ wt, data = mtcars)
model_quad <- lm(mpg ~ wt + I(wt^2), data = mtcars)
summary(model_basic)
summary(model_quad)
```
> Compare R² and coefficients to see if the quadratic model fits better.

### 5. Use `vcovHC()` and `rq()`
```r
coeftest(model_basic, vcov = vcovHC(model_basic, type = "HC1"))
rq_model <- rq(mpg ~ wt, data = mtcars, tau = 0.5)
summary(rq_model)
```
> Tests robustness of coefficient estimates — useful for skewed or noisy data.

### 6. Design your own robustness check
```r
model_small_engines <- lm(mpg ~ wt, data = subset(mtcars, cyl <= 6))
summary(model_small_engines)
```
> Shows what happens when excluding larger engine cars — a simple sensitivity analysis.

---

## 🔍 Reflect and Extend — Sample Responses

- **Why is it dangerous to treat all variables the same way?**
  > Because ordinal, categorical, and continuous variables have different properties and imply different meanings in models.

- **What are the trade-offs between simplicity and complexity?**
  > Simple models are interpretable and generalizable, while complex models can capture more nuanced patterns but may overfit.

- **What other robustness checks could you imagine?**
  > Run models across random subsamples, remove top/bottom quantiles, or check consistency across subsets like transmission type.
