---
title: "Lesson 4: Variables, Transformations, and Robustness in Regression"
author: "Anna Josephson"
date: "2025-06-23"
output: html_document
---

## Package Used

A package is a collection of R functions and datasets designed to make certain tasks easier. You need to install and load a package before using it.

- `arrow`
A package that allows R to read and write data in Parquet format, which is a fast and compact way to store data.

- **Base R**
We also used several functions that are part of base R (which means you don’t need to install or load any extra packages). These include:
  - `lm()` – fits a linear regression model
  -`residuals()` – returns model residuals (prediction errors)
  - `hist()` – creates a histogram to visualize distributions
  - `plot()` – draws basic plots, like scatterplots or residuals
  - `log()` / `log10()` – applies logarithmic transformation
  - `scale()` – standardizes variables (mean = 0, SD = 1)
  - `ifelse()` – creates binary (0/1) variables
  - `which.max()` – finds the position of the maximum value
  - `data.frame()` – creates a data table
  - `factor()` / `ordered()` – defines categorical and ordinal variables
  - `summary()` – provides model output and basic descriptive statistics

- `mtcars` 

A built-in R dataset containing fuel consumption and design specifications for 32 car models from the 1970s. Commonly used for teaching regression and data analysis.

- `quantreg`
Used for quantile regression, which estimates the median (or other percentiles) instead of the mean. Helpful when your data has outliers or is skewed.
  - `rq()` – runs a quantile regression (e.g., median regression)

- `sandwich`
Provides tools for computing robust standard errors, which protect against problems like heteroskedasticity (non-constant variance of residuals).
  - `vcovHC()` – calculates heteroskedasticity-consistent (robust) variance-covariance matrix

- `ggplot2`
A package for data visualization. It lets you make elegant, customizable charts and graphs.


## Learning Objectives

By the end of this lesson, you should be able to:

1. Distinguish between types of variables and understand when to use each.
2. Understand the role of error distributions in regression.
3. Know when to use mean vs. median and why that matters.
4. Recognize why and when transformations like standardization or logs are appropriate.
5. Understand situations where OLS may be insufficient.
6. Explain what robustness checks are and why we do them.

---


## Part 1: Understanding Variables

In economics, variables are how we measure the world. Choosing the right type of variable affects your models and interpretation.

### Types of Variables

- **Categorical Variables**
  - Represent categories or groups
  - Example: Gender (Male, Female), Region (North, South)
  - Use in regression: Convert to dummy variables (0/1)
  - In R: Use `factor()` to turn them into categories.


```{r 1}
region <- factor(c("North", "South", "North", "South"))
dummies <- model.matrix(~ region)[, -1]  # Creates dummy variables
```

- **Ordinal Variables**
  - Categories with a meaningful order
  - Example: Education level (High school < College < Graduate)
  - In R: Use `ordered()` to capture this ranking.
  - Use: Sometimes treated as numeric, but caution is needed - can you give an example of why this might be? 

```{r 2}
education <- ordered(c("High school", "College", "Graduate"),
                     levels = c("High school", "College", "Graduate"))
```

Important: Just because they have an order doesn't mean they behave like numbers. For example, the "gap" between High school and College isn't necessarily the same as College to Graduate.


- **Likert-scale Variables**
  - A special case of ordinal variables
  - Example: Agreement levels (Strongly Disagree to Strongly Agree)
  - Use: Often treated as numeric in practice, but interpretation must consider ordering

```{r 3}
likert <- ordered(c("Disagree", "Neutral", "Agree", "Strongly Agree"),
                  levels = c("Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"))
```


- **Continuous Variables**
  - Numeric and can take any value within a range
  - Example: Income, Age, Height.

```{r 4}
income <- c(32000, 45000, 51000, 39000)
```

- **Binary Variables**
  - Only two outcomes (0 or 1)
  - A course, but often essential, measure 

```{r 5}
employed <- c(1, 0, 1, 1)
```

Binary variables are super useful in modeling (like "Did someone graduate?")
They let us ask clear yes/no questions in our data.

---

## Part 2: Distributions and Error Terms

### About the `mtcars` Dataset

The `mtcars` dataset is a built-in dataset in R that contains data about fuel consumption and design aspects of 32 different car models from the 1970s. It includes variables such as miles per gallon (`mpg`), number of cylinders (`cyl`), horsepower (`hp`), weight (`wt`), and others. This dataset is often used for demonstrating statistical methods, especially linear regression.

Variables in mtcars:

- `mpg`
Miles per gallon – a measure of fuel efficiency.

- `cyl`
Number of cylinders in the engine (usually 4, 6, or 8).

- `disp`
Displacement – engine size in cubic inches.

- `hp`
Gross horsepower – engine power output.

- `drat`
Rear axle ratio – affects engine performance.

- `wt`
Weight of the car (in 1,000 lbs).

- `qsec`
Quarter-mile time – time (in seconds) to travel 1/4 mile from a standstill.

- `vs`
Engine shape: 0 = V-shaped, 1 = straight (in-line).

- `am`
Transmission: 0 = automatic, 1 = manual.

- `gear`
Number of forward gears.

- `carb`
Number of carburetors.



### Why Error Distribution Matters

When you run a regression, the model can’t predict perfectly. The difference between what the model predicts and the actual value is called the error or residual.

OLS = Ordinary Least Squares (standard linear regression). Assumes:

Errors are normally distributed

Errors have constant variance (this is called homoskedasticity)

You can visualize errors like this:

```{r 6}
model <- lm(mpg ~ wt, data = mtcars)
residuals <- residuals(model)
hist(residuals, breaks = 10, main = "Histogram of Residuals")
```

This helps check if your errors follow a nice bell-shaped curve (normal distribution).

#### ** A Bit More on Error and Standard Errors (SE) **

**Residuals vs. Standard Errors**


- Residuals: The difference between your model’s prediction and the actual value for each data point.
  - These tell you how well your model fits each observation.
  -You can plot them to check patterns or problems (e.g., nonlinearity, heteroskedasticity).

- Standard Error: Measures the precision of your coefficient estimates.
  - A smaller SE means more confidence in your estimate.
  - SE is used to calculate confidence intervals and p-values in model summaries.

**Example**:

If your model says `b = -5` with `SE = 1.2`, that’s a pretty precise estimate.

But if `SE = 4.9`, it’s harder to be confident the effect is real.


**What Affects Standard Errors?**

Sample size: More data → smaller SE

Variance of the residuals: More noisy data → larger SE

Multicollinearity (when predictors are highly correlated): Inflates SE


**Visualizing SE: Confidence Intervals**

Try plotting a model with ggplot2 to visualize standard error bands:

```{r 7}
library(ggplot2)

ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE)

```

The shaded band is the confidence interval around the regression line, based on standard error. If it’s wide, your predictions are less certain.

**Why Does This Matter?**

Understanding SE helps you avoid overconfidence in your results.

You may have a big coefficient — but is it statistically different from zero?

A coefficient isn’t “real” just because it’s large; it must also be precise.



### Mean vs. Median in Regression

  - **Mean** is affected by outliers (like one really rich person skewing income).
  - **Median** is more stable if your data has extreme values.

- **OLS** (mean-based):

```{r 8}
ols_model <- lm(mpg ~ wt, data = mtcars)
summary(ols_model)
```

- **Median Regression** (quantile regression):

```{r 9}
install.packages("quantreg", repos = "https://cloud.r-project.org/")
library(quantreg)
median_model <- rq(mpg ~ wt, data = mtcars, tau = 0.5)
summary(median_model)
```

---

## Part 3: Transforming Variables

Sometimes your raw data isn't ideal for analysis. Transformation helps:

- Improve model accuracy

- Make patterns easier to see

- Make variables more comparable

- Handle skewed or extreme data

We’ll focus on two common transformations:

1. Standarization

2. Log Transformation

### Standardization

What it does:
Standardization scales your data so:

- Mean = 0

- Standard deviation = 1

This makes values comparable, especially when your dataset contains variables that are:

- On different scales (e.g., age in years vs. income in dollars)

- Used in models together

**Example**:

You have two variables:

`income` (in dollars): e.g., 32000; 45000; 51000; 39000

`education_years`: e.g., 12; 16; 18; 14

If you put both into a model, income will dominate because it’s measured in bigger units.

Fix by standardizing:

```{r 10}

df <- data.frame(
  income = c(32000, 45000, 51000, 39000),
  education_years = c(12, 16, 18, 14))

df$income_std <- scale(df$income)
df$education_std <- scale(df$education_years)
```

Now both are on the same unitless scale (Z-scores), and your regression treats them fairly.

When to standardize:

- When combining different units (e.g. height, weight, dollars)

- Before using algorithms that are sensitive to scale (e.g., clustering, regression)

- To improve model interpretability

Let's do it:

```{r 11}
mtcars$wt_std <- scale(mtcars$wt)
```

Also, this line of code standardizes the `wt` variable in the `mtcars` dataset and saves it as a new column called `wt_std`.

- `wt` stands for weight of the car, measured in 1,000 pounds.

- `scale(mtcars$wt)` transforms the values into Z-scores: it subtracts the mean and divides by the standard deviation.

- The result is a standardized version of car weight, with a mean of 0 and standard deviation of 1.

This allows you to compare wt fairly with other variables on different scales or include it in a regression model without it dominating due to larger numeric values.


### Log transformation

**What it does**:
It compresses large numbers while keeping their order.

Helps when some values are huge and others are tiny.

Turns multiplicative relationships into additive ones (very helpful in modeling).

**Example**: 

Imagine you're analyzing population by region, and you make a barplot:

```{r 12}
population <- c(800, 1200, 50000, 600, 90000)
barplot(population, names.arg = c("A", "B", "C", "D", "E"))
``` 

This plot will be hard to read because:

One or two bars are so big, the others look flat.

The difference between counties or cities is too extreme.

Fix with a log transformation:

```{r 13}
log_pop <- log10(population)
barplot(log_pop, names.arg = c("A", "B", "C", "D", "E"))
```
Now the bars are more evenly scaled, and patterns become easier to compare.

When to use log transformation:

- Economic data: income, GDP, population

- Health data: hospital bills, medical cases

- Any data with large ranges

So, let's do that for our data:

```{r 14}
mtcars$log_wt <- log(mtcars$wt)
```


If your data has zeros, use:

```{r 15}
mtcars$log_wt_safe <- log(mtcars$wt + 1)
```

This line creates a new variable + `log_wt_safe` by applying the natural logarithm to `wt + 1`.

- Adding 1 before taking the log is a common trick to handle zeros or very small values in your data because `log(0)` is undefined (it causes an error).

- Although `mtcars$wt` doesn’t have zeros, this approach is useful when your variable may contain zero values.

- This ensures the logarithm transformation won’t fail and keeps all values positive before transformation.



---

## Part 4: When OLS Isn’t Enough

OLS: 

- Finds a line that minimizes squared errors

- Assumes a linear relationship

- Assumes errors are evenly spread (homoskedastic)

But real-world data isn’t always this clean. Let’s see why you might need more than OLS.

### 1. Nonlinear Relationships

Imagine you plot weight vs. fuel efficiency (MPG):


```{r 16}
plot(mtcars$wt, mtcars$mpg)
```

You might see a curve, not a straight line.

OLS assumes a line like this:


```r
mpg = a + b*wt
```

But if the data curves, the model misses the pattern.

Fix with a quadratic term:

```{r 17}
model_quad <- lm(mpg ~ wt + I(wt^2), data = mtcars)
summary(model_quad)
```

This lets the line bend (a parabola). It now fits your data better.

### 2. Heteroskedasticity (Unequal Spread of Errors)

OLS assumes that errors (residuals) have the same size all across your data.

**Problem**:

In a scatterplot of residuals, maybe the points spread out as x increases. For this, we need the residuals and the fitted values of the model. 


```r {r 18}
# Fit a simple linear regression model
model <- lm(mpg ~ wt, data = mtcars)

# Extract residuals and fitted values from the model
residuals <- residuals(model)
fitted_values <- fitted(model)
```

Plot residuals vs. fitted values to check for heteroskedasticity

```{r 19}
plot(model$fitted.values, model$residuals)
```

If the plot shows a fan shape, your model has heteroskedasticity.

**What this code does**:

Fits a linear regression model predicting miles per gallon (`mpg`) based on car weight (`wt`) using the built-in mtcars dataset.

Extracts residuals (the differences between observed and predicted values) and fitted values (the predicted `mpg` values).

Plots residuals against fitted values to visually inspect whether the spread of residuals is constant across all predicted values.

OLS regression assumes that residuals have constant variance (called homoskedasticity). If the residuals fan out or get larger/smaller as fitted values increase, this indicates heteroskedasticity, meaning the error variance changes and OLS assumptions are violated.

By looking at this plot, you can get a quick visual check if your model meets this assumption or if you need more advanced techniques.


Fix with robust standard errors:


```{r 20}
install.packages("sandwich", repos = "https://cloud.r-project.org/")
install.packages("lmtest", repos = "https://cloud.r-project.org/")
library(sandwich)
library(lmtest)
coeftest(model, vcov = vcovHC(model, type = "HC1"))
```

This adjusts your confidence in the estimates without changing the model.



### 3. Binary Outcomes: Use Logistic Regression

If your variable is yes/no (e.g., passed = 1, failed = 0), OLS isn’t right.

Create a new binary variable `high_mpg` that is 1 if mpg is greater than 20 (good fuel efficiency), and 0 otherwise.

You fit a logistic regression model (´glm´ with `family = binomial`) to predict the probability of high_mpg being 1 based on car weight (wt).

```{r 21}

mtcars$high_mpg <- ifelse(mtcars$mpg > 20, 1, 0)
logit_model <- glm(high_mpg ~ wt, data = mtcars, family = binomial)
summary(logit_model)
```


**Why not use OLS for this?**

If you tried to fit a linear regression (OLS) on a binary variable like `high_mpg`, the predicted values could be:

- Less than 0 (e.g., -0.2), which makes no sense because probability can’t be negative.

- Greater than 1 (e.g., 1.3), which makes no sense because probability can’t exceed 100%.

OLS doesn't restrict predictions to the 0–1 range, so it can produce invalid probabilities.

Use logistic regression:

```{r 22}
logit_model <- glm(high_mpg ~ wt, data = mtcars, family = binomial)
summary(logit_model)
```

Now predictions are between 0 and 1 — perfect for probabilities.


---

## Part 5: Robustness Checks

In econometrics, robustness checks help us test whether our results are reliable or sensitive to certain assumptions. We want to make sure our findings aren't just a fluke of the specific way we ran the model.

### Try Different Specifications

```{r 23}
model_alt <- lm(mpg ~ wt + hp, data = mtcars)
summary(model_alt)
```

**What's happening here?**

- `lm()` is a function in R that stands for linear model.

- `mpg ~ wt + hp` means we are predicting miles per gallon (`mpg`) using weight (`wt`) and horsepower (´hp´).

- `data = mtcars´ means we are using the built-in `mtcars` dataset.

**Why are we doing this?**

We're trying a different specification of the model. Originally, we might have used just `wt` to predict `mpg`. Now, we’re adding hp to see if that changes the results. This helps test if the model is sensitive to what variables we include.

### Try with and without an outlier

```{r 24}
model_no_outlier <- lm(mpg ~ wt, data = mtcars[-which.max(mtcars$wt), ])
summary(model_no_outlier)
```

**What's happening here?**

- `which.max(mtcars$wt)` finds the row with the highest weight — that's likely an outlier (an extreme value).

- `mtcars[-which.max(mtcars$wt), ]` removes that row from the data.

- Then we run the same model: predicting ´mpg´ using ´wt´.

**Why do this?**

Outliers can distort the results of a regression. This check tells us: “Does removing the most extreme car (in terms of weight) change our conclusions?”

### Try Different Estimators

```r
# Already shown: quantile regression, robust SE
```

**What does this mean?**

Instead of just using regular linear regression (`lm()`), we might try:

- **Quantile regression**: Focuses on the median or other quantiles instead of the average.

- **Robust standard errors**: Adjusts for issues like unequal variability in the data (heteroskedasticity).

These are **alternative methods** to see if the main message from your model still holds.


#### **Big Picture**

Each of these steps asks:

"If I change the way I run my model a little, do I still get the same general story?"

If the answer is yes, your model is robust — a good sign.

---

## Let's Practice

1. Create different variable types using `factor()`, `ordered()`, and `numeric`.
2. Plot with `hist()`, `boxplot()`, and compare `mean()` vs `median()`.
3. Apply `log()` and `scale()` to transform data.
4. Run `lm()` and then try adding a quadratic term: `I(x^2)`.
5. Use `vcovHC()` for robust SE, `rq()` for median regression.
6. Discuss a robustness check you would run.

---

## Reflect and Extend

- Why is it dangerous to treat all variables the same way?
- What are the trade-offs between simplicity (OLS) and complexity (nonlinear models)?
- What other robustness checks could you imagine using?

This lesson gives you the building blocks to move from "just running a regression" to **thinking critically about what your model is doing.**
